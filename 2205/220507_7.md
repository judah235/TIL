# TIL



## 2022 05 07(토)


### 1. 텍스트 전처리

 (1) 정수 인코딩

* dictionary
  * 문장 토큰화
     * nltk.tokenize.sent_tokenize()
  * 단어 토큰화
     * nltk.tokenize.word_tokenize()
  * 정제
     * nltk.corpus.stopwords.words('english')
  * 코드 예시
```
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

raw_text = "Uttar Pradesh Chief Minister Yogi Adityanath on Tuesday ordered the suspension of an official for dereliction of duty, days after removing him from the post of director of secondary education following a class 12 English question paper leak. Vinay Pandey was removed as director of secondary education on April 21 and was posted as the director of literacy alternative education, Urdu and oriental languages.A tweet by the chief minister's office read, \"Another strict action by Ch ..\""

sentences = sent_tokenize(raw_text) #문장 토큰화
stop_words=set(stopwords.words('english'))#불용어 가져오기

#단어 토큰화&빈도수
vocab={}
preprocessed_sentence=[]
for sentence in sentences:
    words=word_tokenize(sentence)
    result=[]
    for word in words:
        word=word.lower()
        if word not in stop_words:
            if len(word)>2:
                result.append(word)
                if word not in vocab:
                    vocab[word]=0
                vocab[word]+=1
    preprocessed_sentence.append(result)

#빈도수 내림차순 정렬
vocab_sorted=sorted(vocab.items(),key=lambda x:x[1],reverse=True)

#높은 빈도수 부터 번호 매기기
#1.
word_to_index={}
i=0
for (word,fre) in vocab_sorted:
    if fre>1:
        i+=1
        word_to_index[word]=i
#2.
word_to_index={word[0]:i+1 for i,word in enumerate(vocab)} #dictionary도 가능   

#인덱스가 4초과인 단어 제거
n=4
words_fre=[word for word,i in word_to_index.items() if i>n]
for w in words_fre:
    del word_to_index[w]

#빈도수가 적은 단어들을 위해 OOV(Out-Of-Vocabulary)추가
word_to_index['OOV']=len(word_to_index)+1

#정수 인코딩

encoded_stences=[]
for sentence in preprocessed_sentence:
    encoded_stence=[]
    for word in sentence:
        try:
            encoded_stence.append(word_to_index[word])
        except KeyError:
            encoded_stence.append(word_to_index['OOV'])
            #encoded_stence.append(len(word_to_index)+1)
    encoded_stences.append(encoded_stence)
```

* collections.Counter()
  * 코드 예시 
```
#문장마다 정제화가 끝난 리스트
preprocessed_sentence

#차원 낮추기
all_words_list=sum(preprocessed_sentence,[])

#빈도수 카운트
vocab=Counter(all_words_list)

#빈도수가 높은 상위 4개 단어만 저장
n=4
vocab=vocab.most_common(n)

#이후 순서는 dictionary와 동일
```

* nltk.Freqdist()
  * 코드 예시
```
#문장마다 정제화가 끝난 리스트
preprocessed_sentence

vocab=FreqDist(sum(preprocessed_sentence,[])) #or np.hstack

#이후 순서는 dictionary와 동일
```

* tensorflow.keras.preprocessing.text.Tokenizer()
  * 코드 예시
```
#문장마다 정제화가 끝난 리스트
preprocessed_sentence

#빈도수 상위 4개만 사용, OOV 설정
tokenizer=Tokenizer(num_words=4+2,oov_token='OOV')

#빈도수 내림차순으로 번호 매기기
tokenizer().fit_on_texts(preprocessed_sentence)

# 정수 인코딩
tokenizer.texts_to_sequences(preprocessed_sentence)
```