# TIL



## 2022 05 07(토)


### 1. 텍스트 전처리

(1) 정수 인코딩

* dictionary
  * 문장 토큰화
     * nltk.tokenize.sent_tokenize()
  * 단어 토큰화
     * nltk.tokenize.word_tokenize()
  * 정제
     * nltk.corpus.stopwords.words('english')
  * 코드 예시
```
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

raw_text = "Uttar Pradesh Chief Minister Yogi Adityanath on Tuesday ordered the suspension of an official for dereliction of duty, days after removing him from the post of director of secondary education following a class 12 English question paper leak. Vinay Pandey was removed as director of secondary education on April 21 and was posted as the director of literacy alternative education, Urdu and oriental languages.A tweet by the chief minister's office read, \"Another strict action by Ch ..\""

sentences = sent_tokenize(raw_text) #문장 토큰화
stop_words=set(stopwords.words('english'))#불용어 가져오기

#단어 토큰화&빈도수
vocab={}
preprocessed_sentence=[]
for sentence in sentences:
    words=word_tokenize(sentence)
    result=[]
    for word in words:
        word=word.lower()
        if word not in stop_words:
            if len(word)>2:
                result.append(word)
                if word not in vocab:
                    vocab[word]=0
                vocab[word]+=1
    preprocessed_sentence.append(result)

#빈도수 내림차순 정렬
vocab_sorted=sorted(vocab.items(),key=lambda x:x[1],reverse=True)

#높은 빈도수 부터 번호 매기기
#1.
word_to_index={}
i=0
for (word,fre) in vocab_sorted:
    if fre>1:
        i+=1
        word_to_index[word]=i
#2.
word_to_index={word[0]:i+1 for i,word in enumerate(vocab)} #dictionary도 가능   

#인덱스가 4초과인 단어 제거
n=4
words_fre=[word for word,i in word_to_index.items() if i>n]
for w in words_fre:
    del word_to_index[w]

#빈도수가 적은 단어들을 위해 OOV(Out-Of-Vocabulary)추가
word_to_index['OOV']=len(word_to_index)+1

#정수 인코딩

encoded_stences=[]
for sentence in preprocessed_sentence:
    encoded_stence=[]
    for word in sentence:
        try:
            encoded_stence.append(word_to_index[word])
        except KeyError:
            encoded_stence.append(word_to_index['OOV'])
            #encoded_stence.append(len(word_to_index)+1)
    encoded_stences.append(encoded_stence)
```

* collections.Counter()
  * 코드 예시 
```
#문장마다 정제화가 끝난 리스트
preprocessed_sentence

#차원 낮추기
all_words_list=sum(preprocessed_sentence,[])

#빈도수 카운트
vocab=Counter(all_words_list)

#빈도수가 높은 상위 4개 단어만 저장
n=4
vocab=vocab.most_common(n)

#이후 순서는 dictionary와 동일
```

* nltk.Freqdist()
  * 코드 예시
```
#문장마다 정제화가 끝난 리스트
preprocessed_sentence

vocab=FreqDist(sum(preprocessed_sentence,[])) #or np.hstack

#이후 순서는 dictionary와 동일
```

* tensorflow.keras.preprocessing.text.Tokenizer()
  * 코드 예시
```
#문장마다 정제화가 끝난 리스트
preprocessed_sentence

#빈도수 상위 4개만 사용, OOV 설정
tokenizer=Tokenizer(num_words=4+2,oov_token='OOV')

#빈도수 내림차순으로 번호 매기기
tokenizer().fit_on_texts(preprocessed_sentence)

# 정수 인코딩
tokenizer.texts_to_sequences(preprocessed_sentence)
```

(2) 패딩

* numpy
  * 코드 예시
```
encoded # 정수 인코딩이 완료된 리스트

max_len = max(len(i) for i in encoded) #가장 단어가 많은 문장의 단어 수 찾기
for sentence in encoded:
    #부족한 만큼 뒤에 0 채우기
    while len(sentence) < max_len:
        sentence.append(0)
padod_np=np.array(encoded)
```
* keras
  * ensorflow.keras.preprocessing.sequence.pad_sequences
  * 코드 예시
```
encoded # 정수 인코딩이 완료된 리스트

from tensorflow.keras.preprocessing.sequence import pad_sequences

#부족한 만큼 앞애 0 채우기
padded=pad_sequences(encoded)

#부족한 만큼 뒤에 0채우기
padded=pad_sequences(encoded, padding='post')

#길이(n)를 지정하여 패딩
padded=pad_sequences(encoded, maxlen=n) # 지정 길이보다 긴 문장은 앞 단어가 손실됨

#뒷 단어가 삭제되도록 할 때
padded=pad_sequences(encoded, truncating='post')

#0이 아닌 다른 숫자(n)로 패딩 할 때
padded=pad_sequences(encoded, value=n)
```

(3) 원-핫 인코딩
* tensorflow.keras.utils.to_categorical : 정수화가 된 리스트를 넣으면 인코딩 된 결과를 인덱스로 원-핫 인코딩을 수행한다.